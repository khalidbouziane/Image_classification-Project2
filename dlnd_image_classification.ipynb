{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from mlxtend.preprocessing import one_hot\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "import sklearn\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 4:\n",
      "Image - Min Value: 0 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 8 Name: ship\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPQAUpugBoDBFDwCFKXoAKEzRA0Bh/xsLZO6u02DPIQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2045fd75cf8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 4\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.array(x/255)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "     \n",
    "    return one_hot(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    "If you're finding it hard to dedicate enough time for this course a week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) to build each layer, except \"Convolutional & Max Pooling\" layer.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    "If you would like to get the most of this course, try to solve all the problems without TF Layers.  Let's begin!\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "     \n",
    "    return tf.placeholder(tf.float32, [None,image_shape[0],image_shape[1],image_shape[2]],name=\"x\")\n",
    "     \n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.placeholder(tf.float32,[None,n_classes],name=\"y\") \n",
    "    \n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return  tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "Note: You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.  You're free to use any TensorFlow package for all the other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([conv_ksize[0],\n",
    "                                     conv_ksize[1],\n",
    "                                     x_tensor.get_shape().as_list()[-1],\n",
    "                                     conv_num_outputs]))\n",
    "\n",
    "    b = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "\n",
    "    conv1 = tf.nn.conv2d(x_tensor, W,\n",
    "             strides=[1, conv_strides[0], conv_strides[1], 1],\n",
    "             padding='SAME')\n",
    "\n",
    "    conv1 = tf.nn.bias_add(conv1, b)\n",
    "\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    max_pool_layer = tf.nn.max_pool(conv1, ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "                      strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "                      padding='SAME')\n",
    "\n",
    "    return max_pool_layer\n",
    "                              \n",
    "\"\"\" \n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    data_size = x_tensor.get_shape().as_list()[1]\n",
    "    w = tf.Variable(tf.truncated_normal([data_size, num_outputs], stddev = 0.1 ))\n",
    "    b = tf.Variable(tf.zeros(num_outputs))\n",
    "     \n",
    "    return tf.nn.relu(tf.add(tf.matmul(x_tensor, w), b))\n",
    "   \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.\n",
    "\n",
    "Note: Activation, softmax, or cross entropy shouldn't be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    data_size = x_tensor.get_shape().as_list()[-1]\n",
    "    w = tf.Variable(tf.truncated_normal([data_size, num_outputs],stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros(num_outputs)) \n",
    "    return tf.add(tf.matmul(x_tensor, w), b)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x_tensor = x\n",
    "    conv_num_outputs = 28\n",
    "    conv_ksize = (2, 2)\n",
    "    conv_strides = (2, 2)\n",
    "    pool_ksize = (2, 2)\n",
    "    pool_strides = (2, 2)\n",
    "    conv = conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    num_outputs = 384\n",
    "    fc = fully_conn(flat,num_outputs)\n",
    "    fc = tf.nn.dropout(fc, keep_prob)\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    num_outputs = 10\n",
    "    out = output(fc,num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability,feature_batch,label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    optimizer = session.run(optimizer, feed_dict={x:feature_batch ,\n",
    "                                   y:label_batch ,keep_prob:keep_probability })\n",
    "\n",
    "    pass\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost,feed_dict={x:feature_batch ,\n",
    "                                   y:label_batch ,keep_prob :1.0 })\n",
    "    valid_acc = session.run(accuracy,feed_dict={x:valid_features ,\n",
    "                                   y:valid_labels ,keep_prob :1.0 })\n",
    "    print('loss at {}'.format(loss),'valid_acc at {}'.format(valid_acc))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 1024\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss at 4.032905101776123 valid_acc at 0.1454000025987625\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss at 2.488481283187866 valid_acc at 0.1940000057220459\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss at 2.138284921646118 valid_acc at 0.20280000567436218\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss at 2.053905725479126 valid_acc at 0.26080000400543213\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss at 1.9893690347671509 valid_acc at 0.27000001072883606\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss at 1.9204938411712646 valid_acc at 0.30820000171661377\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss at 1.8480037450790405 valid_acc at 0.32919999957084656\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss at 1.7939996719360352 valid_acc at 0.3490000069141388\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss at 1.7421207427978516 valid_acc at 0.3625999987125397\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss at 1.6853359937667847 valid_acc at 0.38440001010894775\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss at 1.6331409215927124 valid_acc at 0.39500001072883606\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss at 1.5776402950286865 valid_acc at 0.40639999508857727\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss at 1.543411374092102 valid_acc at 0.42260000109672546\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss at 1.48804771900177 valid_acc at 0.4277999997138977\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss at 1.4516336917877197 valid_acc at 0.4375999867916107\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss at 1.4118268489837646 valid_acc at 0.43880000710487366\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss at 1.385055661201477 valid_acc at 0.4453999996185303\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss at 1.341005563735962 valid_acc at 0.45339998602867126\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss at 1.3203825950622559 valid_acc at 0.4652000069618225\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss at 1.2913004159927368 valid_acc at 0.46700000762939453\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss at 1.2666049003601074 valid_acc at 0.4803999960422516\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss at 1.241313099861145 valid_acc at 0.477400004863739\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss at 1.2236695289611816 valid_acc at 0.4803999960422516\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss at 1.1942310333251953 valid_acc at 0.48100000619888306\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss at 1.1750507354736328 valid_acc at 0.49059998989105225\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss at 1.15569007396698 valid_acc at 0.4973999857902527\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss at 1.1314518451690674 valid_acc at 0.49939998984336853\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss at 1.1139206886291504 valid_acc at 0.4952000081539154\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss at 1.093333125114441 valid_acc at 0.5004000067710876\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss at 1.0791746377944946 valid_acc at 0.5031999945640564\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss at 1.054625391960144 valid_acc at 0.5063999891281128\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss at 1.0374740362167358 valid_acc at 0.5095999836921692\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss at 1.0237549543380737 valid_acc at 0.503000020980835\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss at 1.0000869035720825 valid_acc at 0.5144000053405762\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss at 0.9776461124420166 valid_acc at 0.5135999917984009\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss at 0.9670233130455017 valid_acc at 0.5127999782562256\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss at 0.9510319828987122 valid_acc at 0.5167999863624573\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss at 0.9409068822860718 valid_acc at 0.5174000263214111\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss at 0.9166055917739868 valid_acc at 0.5157999992370605\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss at 0.9066233038902283 valid_acc at 0.5175999999046326\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss at 0.8732686638832092 valid_acc at 0.5174000263214111\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss at 0.8690185546875 valid_acc at 0.5224000215530396\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss at 0.8542144298553467 valid_acc at 0.5206000208854675\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss at 0.8289339542388916 valid_acc at 0.5260000228881836\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss at 0.8254445791244507 valid_acc at 0.52920001745224\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss at 0.8079271912574768 valid_acc at 0.5264000296592712\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss at 0.7950868606567383 valid_acc at 0.524399995803833\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss at 0.770221471786499 valid_acc at 0.5297999978065491\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss at 0.7685983777046204 valid_acc at 0.5230000019073486\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss at 0.7490965127944946 valid_acc at 0.5302000045776367\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss at 0.746211588382721 valid_acc at 0.5257999897003174\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss at 0.7402745485305786 valid_acc at 0.5284000039100647\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss at 0.7107263803482056 valid_acc at 0.5360000133514404\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss at 0.7043371796607971 valid_acc at 0.5299999713897705\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss at 0.6949467658996582 valid_acc at 0.5339999794960022\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss at 0.6723551154136658 valid_acc at 0.5350000262260437\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss at 0.6671619415283203 valid_acc at 0.5288000106811523\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss at 0.6577273607254028 valid_acc at 0.5357999801635742\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss at 0.6370887160301208 valid_acc at 0.5293999910354614\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss at 0.6269412636756897 valid_acc at 0.5307999849319458\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss at 0.6236399412155151 valid_acc at 0.5386000275611877\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss at 0.6098945736885071 valid_acc at 0.532800018787384\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss at 0.5914468169212341 valid_acc at 0.5371999740600586\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss at 0.5929297804832458 valid_acc at 0.5342000126838684\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss at 0.5790683031082153 valid_acc at 0.5343999862670898\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss at 0.5739244222640991 valid_acc at 0.5353999733924866\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss at 0.5570411086082458 valid_acc at 0.5311999917030334\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss at 0.5501318573951721 valid_acc at 0.5388000011444092\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss at 0.5336361527442932 valid_acc at 0.5306000113487244\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss at 0.5242476463317871 valid_acc at 0.5307999849319458\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss at 0.5229234099388123 valid_acc at 0.5378000140190125\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss at 0.5164706707000732 valid_acc at 0.5270000100135803\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss at 0.5114234685897827 valid_acc at 0.5383999943733215\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss at 0.4990849196910858 valid_acc at 0.5364000201225281\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss at 0.4949430823326111 valid_acc at 0.534600019454956\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss at 0.4861900806427002 valid_acc at 0.5353999733924866\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss at 0.47936004400253296 valid_acc at 0.5351999998092651\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss at 0.4873126447200775 valid_acc at 0.52920001745224\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss at 0.4718364179134369 valid_acc at 0.5302000045776367\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss at 0.46953967213630676 valid_acc at 0.5270000100135803\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss at 0.47685372829437256 valid_acc at 0.5221999883651733\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss at 0.44684869050979614 valid_acc at 0.5297999978065491\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss at 0.47105178236961365 valid_acc at 0.5252000093460083\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss at 0.4511434733867645 valid_acc at 0.5267999768257141\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss at 0.4608843922615051 valid_acc at 0.5264000296592712\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss at 0.43089261651039124 valid_acc at 0.5134000182151794\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss at 0.43817126750946045 valid_acc at 0.5246000289916992\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss at 0.4194488525390625 valid_acc at 0.5293999910354614\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss at 0.4122622311115265 valid_acc at 0.5342000126838684\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss at 0.4124877452850342 valid_acc at 0.5275999903678894\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss at 0.4030320644378662 valid_acc at 0.5271999835968018\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss at 0.3951341211795807 valid_acc at 0.5270000100135803\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss at 0.3863575756549835 valid_acc at 0.5350000262260437\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss at 0.3699800372123718 valid_acc at 0.5324000120162964\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss at 0.37344005703926086 valid_acc at 0.5339999794960022\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss at 0.3427095413208008 valid_acc at 0.5360000133514404\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss at 0.35445162653923035 valid_acc at 0.534600019454956\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss at 0.3498685359954834 valid_acc at 0.52920001745224\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss at 0.3359103798866272 valid_acc at 0.5361999869346619\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss at 0.33026984333992004 valid_acc at 0.5332000255584717\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss at 3.2888569831848145 valid_acc at 0.1632000058889389\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss at 2.2602484226226807 valid_acc at 0.2176000028848648\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss at 2.077083110809326 valid_acc at 0.23119999468326569\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss at 2.018634080886841 valid_acc at 0.2709999978542328\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss at 1.9630885124206543 valid_acc at 0.2935999929904938\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss at 1.8907978534698486 valid_acc at 0.3190000057220459\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss at 1.8586210012435913 valid_acc at 0.33959999680519104\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss at 1.7660380601882935 valid_acc at 0.3569999933242798\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss at 1.7184251546859741 valid_acc at 0.3799999952316284\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss at 1.7029285430908203 valid_acc at 0.38659998774528503\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss at 1.6456235647201538 valid_acc at 0.40880000591278076\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss at 1.609494686126709 valid_acc at 0.421999990940094\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss at 1.5511572360992432 valid_acc at 0.42820000648498535\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss at 1.5353902578353882 valid_acc at 0.444599986076355\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss at 1.5203070640563965 valid_acc at 0.44859999418258667\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss at 1.4788860082626343 valid_acc at 0.46619999408721924\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss at 1.4416277408599854 valid_acc at 0.4724000096321106\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss at 1.4165093898773193 valid_acc at 0.4765999913215637\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss at 1.397724986076355 valid_acc at 0.48240000009536743\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss at 1.37519371509552 valid_acc at 0.4887999892234802\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss at 1.3641327619552612 valid_acc at 0.4970000088214874\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss at 1.335532307624817 valid_acc at 0.4909999966621399\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss at 1.314633846282959 valid_acc at 0.4970000088214874\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss at 1.3106595277786255 valid_acc at 0.5067999958992004\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss at 1.2743570804595947 valid_acc at 0.5120000243186951\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss at 1.3037523031234741 valid_acc at 0.5149999856948853\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss at 1.260575532913208 valid_acc at 0.5135999917984009\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss at 1.243298888206482 valid_acc at 0.5138000249862671\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss at 1.2357616424560547 valid_acc at 0.5185999870300293\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss at 1.2064085006713867 valid_acc at 0.5281999707221985\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss at 1.2379308938980103 valid_acc at 0.5365999937057495\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss at 1.2081564664840698 valid_acc at 0.5379999876022339\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss at 1.1954100131988525 valid_acc at 0.5303999781608582\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss at 1.1866827011108398 valid_acc at 0.5415999889373779\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss at 1.1492841243743896 valid_acc at 0.5419999957084656\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss at 1.2053736448287964 valid_acc at 0.5490000247955322\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss at 1.1542317867279053 valid_acc at 0.5486000180244446\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss at 1.1313376426696777 valid_acc at 0.5508000254631042\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss at 1.1424598693847656 valid_acc at 0.5460000038146973\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss at 1.0899347066879272 valid_acc at 0.5568000078201294\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss at 1.1580874919891357 valid_acc at 0.5600000023841858\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss at 1.1102516651153564 valid_acc at 0.5583999752998352\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss at 1.094447135925293 valid_acc at 0.5573999881744385\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss at 1.106832504272461 valid_acc at 0.5568000078201294\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss at 1.0498460531234741 valid_acc at 0.5601999759674072\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss at 1.1245778799057007 valid_acc at 0.5691999793052673\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss at 1.0839489698410034 valid_acc at 0.5662000179290771\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss at 1.0604190826416016 valid_acc at 0.5618000030517578\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss at 1.0704379081726074 valid_acc at 0.5594000220298767\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss at 1.008805274963379 valid_acc at 0.5723999738693237\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss at 1.09050714969635 valid_acc at 0.578000009059906\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss at 1.0351899862289429 valid_acc at 0.5717999935150146\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss at 1.0302987098693848 valid_acc at 0.5741999745368958\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss at 1.0343682765960693 valid_acc at 0.5681999921798706\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss at 0.9795922040939331 valid_acc at 0.5777999758720398\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss at 1.0567532777786255 valid_acc at 0.5807999968528748\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss at 1.0062808990478516 valid_acc at 0.5842000246047974\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss at 0.9988460540771484 valid_acc at 0.5767999887466431\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss at 1.0064760446548462 valid_acc at 0.5727999806404114\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss at 0.9472811818122864 valid_acc at 0.5812000036239624\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss at 1.02860426902771 valid_acc at 0.5874000191688538\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss at 0.9697712659835815 valid_acc at 0.5875999927520752\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss at 0.9702863097190857 valid_acc at 0.5889999866485596\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss at 0.9750153422355652 valid_acc at 0.5795999765396118\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss at 0.9073028564453125 valid_acc at 0.5896000266075134\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss at 1.009529948234558 valid_acc at 0.5911999940872192\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss at 0.9391307234764099 valid_acc at 0.5943999886512756\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss at 0.9485365748405457 valid_acc at 0.5839999914169312\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss at 0.9542725086212158 valid_acc at 0.5763999819755554\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss at 0.8897541761398315 valid_acc at 0.5892000198364258\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss at 0.9799618721008301 valid_acc at 0.6011999845504761\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss at 0.9227137565612793 valid_acc at 0.5898000001907349\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss at 0.9205416440963745 valid_acc at 0.5852000117301941\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss at 0.9217121601104736 valid_acc at 0.5838000178337097\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss at 0.8731804490089417 valid_acc at 0.5979999899864197\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss at 0.9559503197669983 valid_acc at 0.5950000286102295\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss at 0.8885655999183655 valid_acc at 0.5956000089645386\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss at 0.8986014127731323 valid_acc at 0.597599983215332\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss at 0.9058027863502502 valid_acc at 0.5896000266075134\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss at 0.8480393886566162 valid_acc at 0.5961999893188477\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss at 0.9264891743659973 valid_acc at 0.6029999852180481\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss at 0.8651936650276184 valid_acc at 0.5968000292778015\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss at 0.8756200075149536 valid_acc at 0.5974000096321106\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss at 0.8830142021179199 valid_acc at 0.5914000272750854\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss at 0.8096553087234497 valid_acc at 0.6015999913215637\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss at 0.9033940434455872 valid_acc at 0.6037999987602234\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss at 0.8493369817733765 valid_acc at 0.6001999974250793\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss at 0.8528918027877808 valid_acc at 0.5997999906539917\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss at 0.8495197296142578 valid_acc at 0.6037999987602234\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss at 0.7849287986755371 valid_acc at 0.602400004863739\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss at 0.8831769227981567 valid_acc at 0.6087999939918518\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss at 0.8226216435432434 valid_acc at 0.6060000061988831\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss at 0.8249028325080872 valid_acc at 0.598800003528595\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss at 0.833122193813324 valid_acc at 0.599399983882904\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss at 0.7666388750076294 valid_acc at 0.6051999926567078\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss at 0.8601764440536499 valid_acc at 0.6039999723434448\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss at 0.8028544187545776 valid_acc at 0.6105999946594238\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss at 0.8016645908355713 valid_acc at 0.6033999919891357\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss at 0.8064939379692078 valid_acc at 0.5961999893188477\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss at 0.7463313937187195 valid_acc at 0.6086000204086304\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss at 0.8474887013435364 valid_acc at 0.6057999730110168\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss at 0.7904816269874573 valid_acc at 0.6083999872207642\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss at 0.7912225723266602 valid_acc at 0.6047999858856201\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss at 0.79581218957901 valid_acc at 0.6043999791145325\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss at 0.7343629002571106 valid_acc at 0.6115999817848206\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss at 0.8245626091957092 valid_acc at 0.6086000204086304\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss at 0.7703176736831665 valid_acc at 0.6126000285148621\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss at 0.7701237797737122 valid_acc at 0.6003999710083008\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss at 0.7785263657569885 valid_acc at 0.6082000136375427\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss at 0.7000913619995117 valid_acc at 0.6064000129699707\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss at 0.8054088950157166 valid_acc at 0.6128000020980835\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss at 0.7528027892112732 valid_acc at 0.6075999736785889\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss at 0.7505736351013184 valid_acc at 0.6161999702453613\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss at 0.7559121251106262 valid_acc at 0.6132000088691711\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss at 0.6809751987457275 valid_acc at 0.6161999702453613\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss at 0.7950662970542908 valid_acc at 0.6186000108718872\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss at 0.7297377586364746 valid_acc at 0.6074000000953674\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss at 0.7263655662536621 valid_acc at 0.6140000224113464\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss at 0.7430587410926819 valid_acc at 0.607200026512146\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss at 0.6761033535003662 valid_acc at 0.6074000000953674\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss at 0.766839861869812 valid_acc at 0.6179999709129333\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss at 0.7220481634140015 valid_acc at 0.6096000075340271\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss at 0.6999334096908569 valid_acc at 0.6169999837875366\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss at 0.7230340838432312 valid_acc at 0.6075999736785889\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss at 0.6455007195472717 valid_acc at 0.6191999912261963\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss at 0.7457682490348816 valid_acc at 0.6179999709129333\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss at 0.6934973001480103 valid_acc at 0.6195999979972839\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss at 0.6775221228599548 valid_acc at 0.6173999905586243\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss at 0.7051408886909485 valid_acc at 0.6209999918937683\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss at 0.630214512348175 valid_acc at 0.6230000257492065\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss at 0.7255033254623413 valid_acc at 0.6277999877929688\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss at 0.6741096377372742 valid_acc at 0.6169999837875366\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss at 0.6588367223739624 valid_acc at 0.621999979019165\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss at 0.6857181787490845 valid_acc at 0.6177999973297119\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss at 0.6119499802589417 valid_acc at 0.6237999796867371\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss at 0.7051059007644653 valid_acc at 0.626800000667572\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss at 0.6557561755180359 valid_acc at 0.6209999918937683\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss at 0.6421428918838501 valid_acc at 0.6273999810218811\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss at 0.6695144176483154 valid_acc at 0.6212000250816345\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss at 0.5971534848213196 valid_acc at 0.625\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss at 0.6929391026496887 valid_acc at 0.6255999803543091\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss at 0.639695405960083 valid_acc at 0.6227999925613403\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss at 0.6209104061126709 valid_acc at 0.6287999749183655\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss at 0.6546098589897156 valid_acc at 0.6190000176429749\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss at 0.5881512761116028 valid_acc at 0.6218000054359436\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss at 0.6840749382972717 valid_acc at 0.6299999952316284\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss at 0.6212865114212036 valid_acc at 0.6236000061035156\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss at 0.6134132742881775 valid_acc at 0.625\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss at 0.6381900906562805 valid_acc at 0.6240000128746033\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss at 0.5669094324111938 valid_acc at 0.6304000020027161\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss at 0.650252103805542 valid_acc at 0.6317999958992004\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss at 0.603918194770813 valid_acc at 0.6255999803543091\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss at 0.5913318395614624 valid_acc at 0.6304000020027161\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss at 0.6254284977912903 valid_acc at 0.6208000183105469\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss at 0.550832986831665 valid_acc at 0.629800021648407\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss at 0.638153612613678 valid_acc at 0.629800021648407\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss at 0.5875675082206726 valid_acc at 0.6258000135421753\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss at 0.5750142931938171 valid_acc at 0.6377999782562256\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss at 0.6018805503845215 valid_acc at 0.6209999918937683\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss at 0.5395826101303101 valid_acc at 0.6269999742507935\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss at 0.6307049989700317 valid_acc at 0.6287999749183655\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss at 0.585437536239624 valid_acc at 0.6258000135421753\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss at 0.5559208989143372 valid_acc at 0.6302000284194946\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss at 0.5848773121833801 valid_acc at 0.6258000135421753\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss at 0.5295104384422302 valid_acc at 0.6287999749183655\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss at 0.6124753355979919 valid_acc at 0.6353999972343445\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss at 0.5604268908500671 valid_acc at 0.6255999803543091\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss at 0.541625440120697 valid_acc at 0.628600001335144\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss at 0.5788639783859253 valid_acc at 0.6236000061035156\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss at 0.5246378779411316 valid_acc at 0.6299999952316284\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss at 0.5964052677154541 valid_acc at 0.6317999958992004\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss at 0.5420553088188171 valid_acc at 0.6263999938964844\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss at 0.5362011790275574 valid_acc at 0.6313999891281128\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss at 0.5540555715560913 valid_acc at 0.6258000135421753\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss at 0.49827471375465393 valid_acc at 0.6263999938964844\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss at 0.569799542427063 valid_acc at 0.633400022983551\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss at 0.5268949866294861 valid_acc at 0.6304000020027161\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss at 0.5173590779304504 valid_acc at 0.6294000148773193\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss at 0.5406829714775085 valid_acc at 0.6294000148773193\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss at 0.4890204668045044 valid_acc at 0.629800021648407\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss at 0.5688138008117676 valid_acc at 0.6367999911308289\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss at 0.5154436230659485 valid_acc at 0.6258000135421753\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss at 0.4983570873737335 valid_acc at 0.6345999836921692\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss at 0.5233388543128967 valid_acc at 0.6326000094413757\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss at 0.47974032163619995 valid_acc at 0.6344000101089478\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss at 0.5413210391998291 valid_acc at 0.6385999917984009\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss at 0.5009074211120605 valid_acc at 0.6305999755859375\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss at 0.48339346051216125 valid_acc at 0.6355999708175659\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss at 0.5016695261001587 valid_acc at 0.6327999830245972\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss at 0.46840906143188477 valid_acc at 0.6327999830245972\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss at 0.5236918926239014 valid_acc at 0.6373999714851379\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss at 0.48262274265289307 valid_acc at 0.6302000284194946\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss at 0.4758679270744324 valid_acc at 0.6366000175476074\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss at 0.48735710978507996 valid_acc at 0.6290000081062317\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss at 0.4586807191371918 valid_acc at 0.6308000087738037\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss at 0.5234594345092773 valid_acc at 0.6308000087738037\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss at 0.46459320187568665 valid_acc at 0.6341999769210815\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss at 0.46218419075012207 valid_acc at 0.635200023651123\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss at 0.46713685989379883 valid_acc at 0.6399999856948853\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss at 0.43937963247299194 valid_acc at 0.6287999749183655\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss at 0.5043843388557434 valid_acc at 0.6373999714851379\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss at 0.4628816843032837 valid_acc at 0.6295999884605408\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss at 0.4493662714958191 valid_acc at 0.6305999755859375\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss at 0.4656147062778473 valid_acc at 0.6366000175476074\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss at 0.43141043186187744 valid_acc at 0.6313999891281128\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss at 0.4912394881248474 valid_acc at 0.6376000046730042\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss at 0.4534778892993927 valid_acc at 0.6320000290870667\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss at 0.43065279722213745 valid_acc at 0.63919997215271\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss at 0.44469210505485535 valid_acc at 0.6335999965667725\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss at 0.4134994447231293 valid_acc at 0.6370000243186951\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss at 0.47649019956588745 valid_acc at 0.6377999782562256\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss at 0.4309050440788269 valid_acc at 0.6362000107765198\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss at 0.41744372248649597 valid_acc at 0.6425999999046326\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss at 0.4297768175601959 valid_acc at 0.6358000040054321\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss at 0.4062620997428894 valid_acc at 0.635200023651123\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss at 0.46759963035583496 valid_acc at 0.6384000182151794\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss at 0.42458170652389526 valid_acc at 0.6370000243186951\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss at 0.41168057918548584 valid_acc at 0.6389999985694885\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss at 0.40844669938087463 valid_acc at 0.6412000060081482\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss at 0.38950052857398987 valid_acc at 0.6340000033378601\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss at 0.4547426700592041 valid_acc at 0.6381999850273132\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss at 0.4088759422302246 valid_acc at 0.6287999749183655\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss at 0.40851476788520813 valid_acc at 0.6398000121116638\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss at 0.40281492471694946 valid_acc at 0.6439999938011169\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss at 0.3829830586910248 valid_acc at 0.6398000121116638\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss at 0.4419097900390625 valid_acc at 0.6449999809265137\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss at 0.4008820354938507 valid_acc at 0.635200023651123\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss at 0.39298203587532043 valid_acc at 0.6416000127792358\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss at 0.39619308710098267 valid_acc at 0.6358000040054321\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss at 0.3751329183578491 valid_acc at 0.6322000026702881\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss at 0.4275798499584198 valid_acc at 0.6430000066757202\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss at 0.37712934613227844 valid_acc at 0.6403999924659729\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss at 0.36958789825439453 valid_acc at 0.6406000256538391\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss at 0.37849289178848267 valid_acc at 0.6363999843597412\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss at 0.360423743724823 valid_acc at 0.6366000175476074\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss at 0.4154616892337799 valid_acc at 0.6421999931335449\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss at 0.3832809031009674 valid_acc at 0.6363999843597412\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss at 0.3720586895942688 valid_acc at 0.6377999782562256\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss at 0.37125957012176514 valid_acc at 0.6442000269889832\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss at 0.3454250395298004 valid_acc at 0.6330000162124634\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss at 0.4023837745189667 valid_acc at 0.6417999863624573\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss at 0.36695072054862976 valid_acc at 0.6340000033378601\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss at 0.3619621694087982 valid_acc at 0.6389999985694885\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss at 0.36402344703674316 valid_acc at 0.6388000249862671\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss at 0.35116446018218994 valid_acc at 0.6371999979019165\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss at 0.3960341513156891 valid_acc at 0.6416000127792358\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss at 0.34844866394996643 valid_acc at 0.633400022983551\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss at 0.3488682806491852 valid_acc at 0.6394000053405762\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss at 0.3522435128688812 valid_acc at 0.6388000249862671\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss at 0.3381139039993286 valid_acc at 0.635200023651123\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss at 0.3797178864479065 valid_acc at 0.6416000127792358\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss at 0.35375210642814636 valid_acc at 0.6398000121116638\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss at 0.35226401686668396 valid_acc at 0.6373999714851379\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss at 0.3459011912345886 valid_acc at 0.6370000243186951\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss at 0.33006751537323 valid_acc at 0.6345999836921692\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss at 0.36879536509513855 valid_acc at 0.6442000269889832\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss at 0.3558209538459778 valid_acc at 0.6373999714851379\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss at 0.34243524074554443 valid_acc at 0.6384000182151794\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss at 0.331717848777771 valid_acc at 0.628000020980835\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss at 0.31488898396492004 valid_acc at 0.6385999917984009\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss at 0.3722875118255615 valid_acc at 0.644599974155426\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss at 0.333415150642395 valid_acc at 0.6399999856948853\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss at 0.33865493535995483 valid_acc at 0.63919997215271\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss at 0.3337196111679077 valid_acc at 0.6320000290870667\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss at 0.3097197711467743 valid_acc at 0.6335999965667725\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss at 0.3595433831214905 valid_acc at 0.6420000195503235\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss at 0.33231687545776367 valid_acc at 0.6370000243186951\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss at 0.3294453024864197 valid_acc at 0.6413999795913696\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss at 0.3291046619415283 valid_acc at 0.6245999932289124\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss at 0.2980155646800995 valid_acc at 0.6388000249862671\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss at 0.36029839515686035 valid_acc at 0.6363999843597412\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss at 0.3257475793361664 valid_acc at 0.6370000243186951\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss at 0.3157445788383484 valid_acc at 0.6385999917984009\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss at 0.3128381371498108 valid_acc at 0.6349999904632568\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss at 0.2803693413734436 valid_acc at 0.6371999979019165\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss at 0.33574238419532776 valid_acc at 0.6389999985694885\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss at 0.30683526396751404 valid_acc at 0.6434000134468079\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss at 0.3122059404850006 valid_acc at 0.6353999972343445\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss at 0.29854536056518555 valid_acc at 0.633400022983551\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss at 0.28234410285949707 valid_acc at 0.6363999843597412\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss at 0.3355092704296112 valid_acc at 0.6373999714851379\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss at 0.31411096453666687 valid_acc at 0.6431999802589417\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss at 0.32406067848205566 valid_acc at 0.6363999843597412\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss at 0.29379943013191223 valid_acc at 0.633400022983551\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss at 0.27092233300209045 valid_acc at 0.6345999836921692\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss at 0.3109312057495117 valid_acc at 0.6399999856948853\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss at 0.30206093192100525 valid_acc at 0.6448000073432922\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss at 0.3089408874511719 valid_acc at 0.6398000121116638\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss at 0.2802533507347107 valid_acc at 0.6358000040054321\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss at 0.259625107049942 valid_acc at 0.6402000188827515\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss at 0.30741086602211 valid_acc at 0.6434000134468079\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss at 0.28571563959121704 valid_acc at 0.6421999931335449\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss at 0.28583770990371704 valid_acc at 0.6385999917984009\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss at 0.27247926592826843 valid_acc at 0.6344000101089478\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss at 0.2576983571052551 valid_acc at 0.633400022983551\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss at 0.3013722598552704 valid_acc at 0.6430000066757202\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss at 0.28801167011260986 valid_acc at 0.6389999985694885\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss at 0.28098008036613464 valid_acc at 0.6366000175476074\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss at 0.26143577694892883 valid_acc at 0.6366000175476074\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss at 0.24758711457252502 valid_acc at 0.6363999843597412\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss at 0.289907842874527 valid_acc at 0.6439999938011169\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss at 0.28102266788482666 valid_acc at 0.6394000053405762\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss at 0.2722183167934418 valid_acc at 0.6388000249862671\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss at 0.2614717185497284 valid_acc at 0.6359999775886536\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss at 0.23274779319763184 valid_acc at 0.6406000256538391\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss at 0.28089118003845215 valid_acc at 0.642799973487854\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss at 0.27162909507751465 valid_acc at 0.640999972820282\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss at 0.25960126519203186 valid_acc at 0.6416000127792358\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss at 0.25121617317199707 valid_acc at 0.6326000094413757\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss at 0.22972941398620605 valid_acc at 0.6417999863624573\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss at 0.27136391401290894 valid_acc at 0.6444000005722046\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss at 0.2703905701637268 valid_acc at 0.6402000188827515\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss at 0.2535715401172638 valid_acc at 0.6366000175476074\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss at 0.244281604886055 valid_acc at 0.6359999775886536\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss at 0.22210916876792908 valid_acc at 0.6424000263214111\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss at 0.26954057812690735 valid_acc at 0.6448000073432922\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss at 0.2519465982913971 valid_acc at 0.6384000182151794\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss at 0.23871178925037384 valid_acc at 0.6452000141143799\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss at 0.2314738780260086 valid_acc at 0.6358000040054321\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss at 0.20545202493667603 valid_acc at 0.6420000195503235\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss at 0.2576272189617157 valid_acc at 0.642799973487854\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss at 0.24844729900360107 valid_acc at 0.6362000107765198\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss at 0.23702387511730194 valid_acc at 0.6434000134468079\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss at 0.2258647084236145 valid_acc at 0.6366000175476074\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss at 0.2028466910123825 valid_acc at 0.6442000269889832\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss at 0.2517447769641876 valid_acc at 0.642799973487854\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss at 0.24052977561950684 valid_acc at 0.6398000121116638\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss at 0.223949134349823 valid_acc at 0.6373999714851379\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss at 0.2177809178829193 valid_acc at 0.63919997215271\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss at 0.1983233094215393 valid_acc at 0.6452000141143799\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss at 0.2375180870294571 valid_acc at 0.6456000208854675\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss at 0.22896409034729004 valid_acc at 0.6345999836921692\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss at 0.21682476997375488 valid_acc at 0.6389999985694885\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss at 0.21361929178237915 valid_acc at 0.6353999972343445\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss at 0.19320204854011536 valid_acc at 0.645799994468689\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss at 0.23242999613285065 valid_acc at 0.6453999876976013\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss at 0.22213594615459442 valid_acc at 0.6373999714851379\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss at 0.2177407145500183 valid_acc at 0.6355999708175659\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss at 0.20291708409786224 valid_acc at 0.6320000290870667\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss at 0.18842889368534088 valid_acc at 0.6435999870300293\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss at 0.22944463789463043 valid_acc at 0.6474000215530396\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss at 0.222861185669899 valid_acc at 0.6304000020027161\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss at 0.21074536442756653 valid_acc at 0.6421999931335449\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss at 0.20200799405574799 valid_acc at 0.6348000168800354\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss at 0.18498463928699493 valid_acc at 0.6417999863624573\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss at 0.22462500631809235 valid_acc at 0.6462000012397766\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss at 0.21326453983783722 valid_acc at 0.6341999769210815\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss at 0.20059791207313538 valid_acc at 0.6403999924659729\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss at 0.1943449229001999 valid_acc at 0.6312000155448914\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss at 0.1764010190963745 valid_acc at 0.6434000134468079\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss at 0.215165376663208 valid_acc at 0.6430000066757202\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss at 0.19720856845378876 valid_acc at 0.6371999979019165\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss at 0.19545409083366394 valid_acc at 0.6407999992370605\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss at 0.19423499703407288 valid_acc at 0.6341999769210815\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss at 0.17063382267951965 valid_acc at 0.6424000263214111\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss at 0.20942559838294983 valid_acc at 0.6425999999046326\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss at 0.19734373688697815 valid_acc at 0.6330000162124634\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss at 0.19001474976539612 valid_acc at 0.6399999856948853\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss at 0.18561406433582306 valid_acc at 0.633400022983551\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss at 0.15995709598064423 valid_acc at 0.6435999870300293\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss at 0.20996086299419403 valid_acc at 0.6424000263214111\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss at 0.18770311772823334 valid_acc at 0.6345999836921692\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss at 0.1808665245771408 valid_acc at 0.6377999782562256\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss at 0.18123070895671844 valid_acc at 0.635200023651123\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss at 0.15435683727264404 valid_acc at 0.6366000175476074\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss at 0.19185298681259155 valid_acc at 0.6424000263214111\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss at 0.1853901743888855 valid_acc at 0.6377999782562256\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss at 0.18297310173511505 valid_acc at 0.6384000182151794\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss at 0.17159348726272583 valid_acc at 0.6395999789237976\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss at 0.15321879088878632 valid_acc at 0.6388000249862671\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss at 0.191399484872818 valid_acc at 0.6388000249862671\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss at 0.182443767786026 valid_acc at 0.6381999850273132\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss at 0.1805584579706192 valid_acc at 0.646399974822998\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss at 0.17078924179077148 valid_acc at 0.6394000053405762\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss at 0.1496506631374359 valid_acc at 0.6403999924659729\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss at 0.18373730778694153 valid_acc at 0.6373999714851379\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss at 0.178961843252182 valid_acc at 0.6362000107765198\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss at 0.1746681183576584 valid_acc at 0.6425999999046326\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss at 0.16557928919792175 valid_acc at 0.6420000195503235\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss at 0.1425672471523285 valid_acc at 0.6439999938011169\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss at 0.18157705664634705 valid_acc at 0.6394000053405762\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss at 0.17313510179519653 valid_acc at 0.6345999836921692\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss at 0.16684092581272125 valid_acc at 0.6420000195503235\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss at 0.15639634430408478 valid_acc at 0.6385999917984009\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss at 0.14423510432243347 valid_acc at 0.6313999891281128\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss at 0.17880092561244965 valid_acc at 0.6377999782562256\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss at 0.16749481856822968 valid_acc at 0.6353999972343445\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss at 0.168819859623909 valid_acc at 0.6421999931335449\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss at 0.15543505549430847 valid_acc at 0.6385999917984009\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss at 0.1342093050479889 valid_acc at 0.6407999992370605\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss at 0.1772737056016922 valid_acc at 0.6363999843597412\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss at 0.1632453203201294 valid_acc at 0.6353999972343445\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss at 0.16301259398460388 valid_acc at 0.6430000066757202\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss at 0.14708904922008514 valid_acc at 0.6394000053405762\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss at 0.13451151549816132 valid_acc at 0.631600022315979\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss at 0.1737985461950302 valid_acc at 0.6308000087738037\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss at 0.15919607877731323 valid_acc at 0.635200023651123\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss at 0.15372434258460999 valid_acc at 0.6406000256538391\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss at 0.14817161858081818 valid_acc at 0.6376000046730042\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss at 0.12796682119369507 valid_acc at 0.6376000046730042\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss at 0.16849030554294586 valid_acc at 0.6313999891281128\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss at 0.1565827876329422 valid_acc at 0.6355999708175659\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss at 0.14932093024253845 valid_acc at 0.644599974155426\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss at 0.14445708692073822 valid_acc at 0.6417999863624573\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss at 0.1222560927271843 valid_acc at 0.6348000168800354\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss at 0.16244344413280487 valid_acc at 0.6313999891281128\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss at 0.14773878455162048 valid_acc at 0.6326000094413757\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss at 0.14434696733951569 valid_acc at 0.6442000269889832\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss at 0.13863113522529602 valid_acc at 0.6388000249862671\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss at 0.12252701073884964 valid_acc at 0.6338000297546387\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss at 0.1626383662223816 valid_acc at 0.6344000101089478\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss at 0.14034724235534668 valid_acc at 0.6371999979019165\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss at 0.1466631144285202 valid_acc at 0.6430000066757202\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss at 0.13329795002937317 valid_acc at 0.6470000147819519\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss at 0.12419696897268295 valid_acc at 0.6385999917984009\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss at 0.1499430537223816 valid_acc at 0.6341999769210815\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss at 0.13993465900421143 valid_acc at 0.6322000026702881\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss at 0.1380697786808014 valid_acc at 0.6399999856948853\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss at 0.13447371125221252 valid_acc at 0.640999972820282\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss at 0.11440307646989822 valid_acc at 0.6398000121116638\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss at 0.14570382237434387 valid_acc at 0.6330000162124634\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss at 0.1376836746931076 valid_acc at 0.6367999911308289\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss at 0.1359795331954956 valid_acc at 0.6395999789237976\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss at 0.12415854632854462 valid_acc at 0.640999972820282\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss at 0.1143030971288681 valid_acc at 0.6370000243186951\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss at 0.1474071592092514 valid_acc at 0.6313999891281128\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss at 0.1258242279291153 valid_acc at 0.63919997215271\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss at 0.1304270476102829 valid_acc at 0.6434000134468079\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss at 0.12322362512350082 valid_acc at 0.642799973487854\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss at 0.11110976338386536 valid_acc at 0.6399999856948853\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss at 0.14310865104198456 valid_acc at 0.6322000026702881\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss at 0.1251506358385086 valid_acc at 0.6349999904632568\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss at 0.12819865345954895 valid_acc at 0.6370000243186951\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss at 0.11899623274803162 valid_acc at 0.6380000114440918\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss at 0.10904500633478165 valid_acc at 0.6362000107765198\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss at 0.13688604533672333 valid_acc at 0.6355999708175659\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss at 0.11971013247966766 valid_acc at 0.6359999775886536\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss at 0.12677311897277832 valid_acc at 0.6362000107765198\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss at 0.12150835990905762 valid_acc at 0.6362000107765198\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss at 0.10445933789014816 valid_acc at 0.6402000188827515\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss at 0.13551220297813416 valid_acc at 0.6341999769210815\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss at 0.11924629658460617 valid_acc at 0.6323999762535095\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss at 0.13031113147735596 valid_acc at 0.6338000297546387\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss at 0.11338475346565247 valid_acc at 0.6395999789237976\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss at 0.10328485816717148 valid_acc at 0.6412000060081482\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss at 0.12802422046661377 valid_acc at 0.6345999836921692\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss at 0.10903701931238174 valid_acc at 0.6326000094413757\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss at 0.12193572521209717 valid_acc at 0.6353999972343445\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss at 0.11218602955341339 valid_acc at 0.6355999708175659\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss at 0.10214421898126602 valid_acc at 0.6362000107765198\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss at 0.1258658766746521 valid_acc at 0.6399999856948853\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss at 0.11147959530353546 valid_acc at 0.6355999708175659\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss at 0.11769478768110275 valid_acc at 0.6348000168800354\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss at 0.10789404809474945 valid_acc at 0.6367999911308289\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss at 0.10087259113788605 valid_acc at 0.6362000107765198\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss at 0.12685047090053558 valid_acc at 0.6359999775886536\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss at 0.10778655856847763 valid_acc at 0.6323999762535095\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss at 0.11025389283895493 valid_acc at 0.6370000243186951\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss at 0.10561161488294601 valid_acc at 0.6362000107765198\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss at 0.10446725785732269 valid_acc at 0.6359999775886536\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss at 0.1234326884150505 valid_acc at 0.6373999714851379\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss at 0.10520900040864944 valid_acc at 0.6348000168800354\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss at 0.10896039754152298 valid_acc at 0.6362000107765198\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss at 0.10131531953811646 valid_acc at 0.6341999769210815\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss at 0.09263389557600021 valid_acc at 0.6312000155448914\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss at 0.1189330518245697 valid_acc at 0.6363999843597412\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss at 0.10900484025478363 valid_acc at 0.628000020980835\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss at 0.10969723016023636 valid_acc at 0.6388000249862671\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss at 0.10037027299404144 valid_acc at 0.6367999911308289\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss at 0.09615626186132431 valid_acc at 0.635200023651123\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss at 0.12032705545425415 valid_acc at 0.6348000168800354\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss at 0.09968924522399902 valid_acc at 0.6388000249862671\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss at 0.09929647296667099 valid_acc at 0.6421999931335449\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss at 0.0944494903087616 valid_acc at 0.6331999897956848\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss at 0.0878562182188034 valid_acc at 0.6355999708175659\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss at 0.11538158357143402 valid_acc at 0.6309999823570251\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss at 0.09769661724567413 valid_acc at 0.6290000081062317\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss at 0.09909804165363312 valid_acc at 0.6330000162124634\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss at 0.09270361065864563 valid_acc at 0.6377999782562256\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss at 0.0917801707983017 valid_acc at 0.6302000284194946\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss at 0.1080915704369545 valid_acc at 0.6362000107765198\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss at 0.0937926396727562 valid_acc at 0.6348000168800354\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss at 0.09538112580776215 valid_acc at 0.6384000182151794\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss at 0.09371604770421982 valid_acc at 0.6353999972343445\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss at 0.08453601598739624 valid_acc at 0.63919997215271\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss at 0.11129067838191986 valid_acc at 0.6295999884605408\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss at 0.09274144470691681 valid_acc at 0.6330000162124634\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss at 0.08854539692401886 valid_acc at 0.6353999972343445\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss at 0.09436392039060593 valid_acc at 0.6371999979019165\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss at 0.08829526603221893 valid_acc at 0.6376000046730042\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss at 0.11282163858413696 valid_acc at 0.6331999897956848\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss at 0.09259569644927979 valid_acc at 0.6313999891281128\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss at 0.08843854069709778 valid_acc at 0.6349999904632568\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss at 0.08710178732872009 valid_acc at 0.6380000114440918\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss at 0.08368133008480072 valid_acc at 0.6313999891281128\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss at 0.10538667440414429 valid_acc at 0.6309999823570251\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss at 0.09022291004657745 valid_acc at 0.6322000026702881\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss at 0.08803121745586395 valid_acc at 0.6330000162124634\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss at 0.07883166521787643 valid_acc at 0.6345999836921692\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss at 0.07991231977939606 valid_acc at 0.635200023651123\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6343391239643097\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20400467748>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
